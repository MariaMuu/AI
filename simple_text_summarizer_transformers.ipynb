{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcjVSz4C7P9utziH71X30k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaMuu/AI/blob/main/simple_text_summarizer_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie35HY2o5f-1",
        "outputId": "df84f04d-c8aa-4323-b9f4-b018430ba3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# importing transformers\n",
        "\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dependencies\n",
        "\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "uEK0N1Sg5va6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading summarization pipeline\n",
        "\n",
        "summarizer = pipeline('summarization')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNQKmb_k56aC",
        "outputId": "d1ceecdc-ec59-408f-9a4a-e7173459be3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize our text\n",
        "\n",
        "text = \"\"\"Certainly! Here's a comprehensive and detailed explanation of how neural networks work, encompassing their structure, functionality, training processes, and various architectures:\n",
        "\n",
        "---\n",
        "\n",
        "A neural network is a computational model inspired by the human brain's structure and function. It consists of interconnected units called neurons, organized into layers: an input layer, one or more hidden layers, and an output layer. Each neuron receives input, processes it, and passes the output to the next layer.\n",
        "\n",
        "**Structure and Functionality:**\n",
        "\n",
        "* **Neurons and Layers:** Each neuron in a layer is connected to neurons in adjacent layers through weighted connections. The input layer receives raw data, hidden layers perform computations, and the output layer produces the final result.\n",
        "\n",
        "* **Weights and Biases:** Connections between neurons have associated weights, which determine the strength of the signal transmitted. Each neuron also has a bias value that adjusts the output along with the weighted sum of inputs.\n",
        "\n",
        "* **Activation Functions:** After computing the weighted sum and adding the bias, the result is passed through an activation function. This function introduces non-linearity into the model, enabling it to learn complex patterns. Common activation functions include:\n",
        "\n",
        "  * **Sigmoid:** Outputs values between 0 and 1.\n",
        "  * **Tanh:** Outputs values between -1 and 1.\n",
        "  * **ReLU (Rectified Linear Unit):** Outputs zero for negative inputs and the input itself for positive inputs.([khanacademy.org][1])\n",
        "\n",
        "**Training Process:**\n",
        "\n",
        "* **Forward Propagation:** Input data is passed through the network layer by layer. Each neuron computes its output based on inputs, weights, biases, and the activation function. The final output is compared to the actual target to compute the error.\n",
        "\n",
        "* **Loss Function:** The error between the predicted output and the actual target is quantified using a loss function. Common loss functions include Mean Squared Error for regression tasks and Cross-Entropy Loss for classification tasks.\n",
        "\n",
        "* **Backpropagation:** To minimize the loss, the network adjusts its weights and biases. Backpropagation computes the gradient of the loss function with respect to each weight and bias by applying the chain rule of calculus. This process propagates the error backward through the network.\n",
        "\n",
        "* **Gradient Descent:** Using the gradients computed during backpropagation, the network updates its weights and biases in the direction that reduces the loss. The learning rate determines the size of these updates. Variants like Stochastic Gradient Descent (SGD), Adam, and RMSprop are commonly used optimization algorithms.\n",
        "\n",
        "**Architectures:**\n",
        "\n",
        "* **Feedforward Neural Networks (FNN):** The simplest type, where connections do not form cycles. Data moves in one direction from input to output.([en.wikipedia.org][2])\n",
        "\n",
        "* **Convolutional Neural Networks (CNN):** Specialized for processing grid-like data such as images. They use convolutional layers to extract spatial features, pooling layers to reduce dimensionality, and fully connected layers for classification.([en.wikipedia.org][3])\n",
        "\n",
        "* **Recurrent Neural Networks (RNN):** Designed for sequential data. They have connections that form cycles, allowing information to persist across time steps. Variants like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) address issues like vanishing gradients.\n",
        "\n",
        "* **Residual Neural Networks (ResNet):** Introduce skip connections that allow gradients to flow directly through the network, enabling the training of very deep networks.\n",
        "\n",
        "* **Transformer Networks:** Utilize self-attention mechanisms to process sequential data without recurrence. They have become the foundation for many state-of-the-art models in natural language processing.\n",
        "\n",
        "**Regularization Techniques:**\n",
        "\n",
        "To prevent overfitting, various regularization methods are employed:\n",
        "\n",
        "* **Dropout:** Randomly disables a fraction of neurons during training to prevent co-adaptation.\n",
        "\n",
        "* **L1 and L2 Regularization:** Add penalty terms to the loss function based on the magnitude of weights, encouraging simpler models.\n",
        "\n",
        "* **Early Stopping:** Halts training when performance on a validation set starts to degrade.\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "Neural networks are applied in various domains:\n",
        "\n",
        "* **Computer Vision:** Image classification, object detection, and segmentation.\n",
        "\n",
        "* **Natural Language Processing:** Language translation, sentiment analysis, and text generation.\n",
        "\n",
        "* **Speech Recognition:** Converting spoken language into text.\n",
        "\n",
        "* **Time Series Prediction:** Forecasting stock prices, weather, and other temporal data.\n",
        "\n",
        "* **Healthcare:** Disease diagnosis, medical image analysis, and personalized treatment recommendations.\n",
        "\n",
        "**Challenges and Considerations:**\n",
        "\n",
        "* **Data Requirements:** Neural networks often require large amounts of labeled data for effective training.\n",
        "\n",
        "* **Computational Resources:** Training deep networks can be computationally intensive, necessitating specialized hardware like GPUs.\n",
        "\n",
        "* **Interpretability:** Understanding the decision-making process of neural networks can be challenging, leading to research in explainable AI.\n",
        "\n",
        "* **Hyperparameter Tuning:** Selecting appropriate hyperparameters (e.g., learning rate, number of layers, activation functions) is crucial for model performance and often requires experimentation.\n",
        "\n",
        "**Advancements:**\n",
        "\n",
        "Recent developments have led to more efficient and powerful neural network models:\n",
        "\n",
        "* **Transfer Learning:** Leveraging pre-trained models on new tasks, reducing data and computational requirements.\n",
        "\n",
        "* **Neural Architecture Search (NAS):** Automating the design of neural network architectures for optimal performance.\n",
        "\n",
        "* **Self-Supervised Learning:** Learning representations from unlabeled data, expanding the applicability of neural networks.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NbYuGzYy6Fq7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFJXl3Hi8ABW",
        "outputId": "1c5e58f6-d428-4a16-c939-2a427d4f4455"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i want a summary of 100-500 words\n",
        "\n",
        "summary = summarizer(text, max_length = 500, min_length = 100, do_sample = False, truncation=True)\n"
      ],
      "metadata": {
        "id": "Sa9tU4vP7Adj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the summary text from the list and dictionary structure\n",
        "# and then apply the strip() method\n",
        "\n",
        "cleaned_summary = summary[0]['summary_text'].strip()\n",
        "\n",
        "cleaned_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "sqF6JarZY6tA",
        "outputId": "5a1245d6-3970-4f41-d308-240a417f43ad"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A neural network is a computational model inspired by the human brain's structure and function . It consists of interconnected units called neurons, organized into layers: an input layer, one or more hidden layers, and an output layer . Each neuron receives input, processes it, and passes the output to the next layer . The loss function is quantified using a loss function . The training process propagates the error backward through the network . The network updates its weights and biases in the direction that reduces the loss .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_cleaned_summary = cleaned_summary.replace(' .', '.')\n",
        "final_cleaned_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "mqQgJALLf1lw",
        "outputId": "262013f8-6cb1-47c9-8861-79e06a8814f6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A neural network is a computational model inspired by the human brain's structure and function. It consists of interconnected units called neurons, organized into layers: an input layer, one or more hidden layers, and an output layer. Each neuron receives input, processes it, and passes the output to the next layer. The loss function is quantified using a loss function. The training process propagates the error backward through the network. The network updates its weights and biases in the direction that reduces the loss.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}