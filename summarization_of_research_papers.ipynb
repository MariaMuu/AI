{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWBkwD+Z8DLenLuuD8KArC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaMuu/AI/blob/main/summarization_of_research_papers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDDCYbajbYss",
        "outputId": "161f30ce-e5f6-4c58-a7bf-daf145e72a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=2e4ea122e6cde3fee9e1ce1d1a9a65426b207acc13dd9e18232940173f618782\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "qQQegBALbn9g"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to fetch AI related papers\n",
        "\n",
        "query = 'RAG OR knowledge graphs OR Fine-tune'\n",
        "search = arxiv.Search(\n",
        "    query = query,\n",
        "    max_results = 6,\n",
        "    sort_by = arxiv.SortCriterion.LastUpdatedDate\n",
        ")"
      ],
      "metadata": {
        "id": "FndhURjEbu-C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch papers\n",
        "\n",
        "papers = []\n",
        "for result in search.results():\n",
        "    papers.append({\n",
        "        'published': result.published,\n",
        "        'title': result.title,\n",
        "        'summary': result.summary,\n",
        "        'url': result.pdf_url,\n",
        "        'categories': result.categories\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9764rJKRcTZS",
        "outputId": "72316462-6c48-4006-a5ce-dcdedef2db77"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-3442d87f073d>:4: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to DataFrame\n",
        "\n",
        "df = pd.DataFrame(papers)"
      ],
      "metadata": {
        "id": "Vjxh5KLydhl7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NxgmebsnfefP",
        "outputId": "9257bf55-14bb-4ab4-f3df-4b7107070c8e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  published  \\\n",
              "0 2025-06-05 17:59:26+00:00   \n",
              "1 2025-03-31 13:56:03+00:00   \n",
              "2 2025-06-05 17:48:19+00:00   \n",
              "3 2024-12-05 12:37:27+00:00   \n",
              "4 2025-06-05 17:33:02+00:00   \n",
              "5 2024-10-24 17:56:08+00:00   \n",
              "\n",
              "                                                                                         title  \\\n",
              "0                                                Search Arena: Analyzing Search-Augmented LLMs   \n",
              "1                      Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?   \n",
              "2                  Sample Complexity and Representation Ability of Test-time Scaling Paradigms   \n",
              "3  GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice Question Answering   \n",
              "4   Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning   \n",
              "5               Context is Key: A Benchmark for Forecasting with Essential Textual Information   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         summary  \\\n",
              "0  Search-augmented language models combine web search with Large Language\\nModels (LLMs) to improve response groundedness and freshness. However,\\nanalyzing these systems remains challenging: existing datasets are limited in\\nscale and narrow in scope, often constrained to static, single-turn,\\nfact-checking questions. In this work, we introduce Search Arena, a\\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\\ndiverse intents and languages, and contains full system traces with around\\n12,000 human preference votes. Our analysis reveals that user preferences are\\ninfluenced by the number of citations, even when the cited content does not\\ndirectly support the attributed claims, uncovering a gap between perceived and\\nactual credibility. Furthermore, user preferences vary across cited sources,\\nrevealing that community-driven platforms are generally preferred and static\\nencyclopedic sources are not always appropriate and reliable. To assess\\nperformance across different settings, we conduct cross-arena analyses by\\ntesting search-augmented LLMs in a general-purpose chat environment and\\nconventional LLMs in search-intensive settings. We find that web search does\\nnot degrade and may even improve performance in non-search settings; however,\\nthe quality in search settings is significantly affected if solely relying on\\nthe model's parametric knowledge. We open-sourced the dataset to support future\\nresearch in this direction. Our dataset and code are available at:\\nhttps://github.com/lmarena/search-arena.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                    Low-Resource Languages (LRLs) present significant challenges in natural\\nlanguage processing due to their limited linguistic resources and\\nunderrepresentation in standard datasets. While recent advances in Large\\nLanguage Models (LLMs) and Neural Machine Translation have substantially\\nimproved translation capabilities for high-resource languages, performance\\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\\nresource-constrained scenarios. This paper systematically evaluates current\\nLLMs in 200 languages using the FLORES-200 benchmark and demonstrates their\\nlimitations in LRL translation capability. We also explore alternative data\\nsources, including news articles and bilingual dictionaries, and demonstrate\\nhow knowledge distillation from large pre-trained teacher models can\\nsignificantly improve the performance of small LLMs on LRL translation tasks.\\nFor example, this approach increases EN->LB with the LLM-as-a-Judge score on\\nthe validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine\\ndifferent fine-tuning configurations, providing practical insights on optimal\\ndata scale, training efficiency, and the preservation of generalization\\ncapabilities of models under study.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                   Test-time scaling paradigms have significantly advanced the capabilities of\\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\\ntheoretical understanding of the sample efficiency of various test-time\\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\\nremains limited. In this work, we first establish a separation result between\\ntwo repeated sampling strategies: self-consistency requires\\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\\nbetween the correct and second most likely answers. Next, we present an\\nexpressiveness result for the self-correction approach with verifier feedback:\\nit enables Transformers to simulate online learning over a pool of experts at\\ntest time. Therefore, a single Transformer architecture can provably solve\\nmultiple tasks without prior knowledge of the specific task associated with a\\nuser query, extending the representation theory of Transformers from\\nsingle-task to multi-task settings. Finally, we empirically validate our\\ntheoretical results, demonstrating the practical effectiveness of\\nself-correction methods.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                           Pre-trained Language Models (PLMs) have shown remarkable performances in\\nrecent years, setting a new paradigm for NLP research and industry. The legal\\ndomain has received some attention from the NLP community partly due to its\\ntextual nature. Some tasks from this domain are represented by\\nquestion-answering (QA) tasks. This work explores the legal domain\\nMultiple-Choice QA (MCQA) for a low-resource language. The contribution of this\\nwork is multi-fold. We first introduce JuRO, the first openly available\\nRomanian legal MCQA dataset, comprising three different examinations and a\\nnumber of 10,836 total questions. Along with this dataset, we introduce CROL,\\nan organized corpus of laws that has a total of 93 distinct documents with\\ntheir modifications from 763 time spans, that we leveraged in this work for\\nInformation Retrieval (IR) techniques. Moreover, we are the first to propose\\nLaw-RoG, a Knowledge Graph (KG) for the Romanian language, and this KG is\\nderived from the aforementioned corpus. Lastly, we propose a novel approach for\\nMCQA, Graph Retrieval Augmented by Facts (GRAF), which achieves competitive\\nresults with generally accepted SOTA methods and even exceeds them in most\\nsettings.   \n",
              "4                                                                                                                                                                                                                                                                                    Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\\nConflicts, where retrieved external knowledge contradicts the inherent,\\nparametric knowledge of large language models (LLMs). It adversely affects\\nperformance on downstream tasks such as question answering (QA). Existing\\napproaches often attempt to mitigate conflicts by directly comparing two\\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\\nextraneous or lengthy contexts, ultimately hindering their ability to identify\\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\\nframework with a hierarchical action space that automatically perceives context\\ncomplexity and adaptively decomposes each knowledge source into a sequence of\\nfine-grained comparisons. These comparisons are represented as actionable\\nsteps, enabling reasoning beyond the superficial context. Through extensive\\nexperiments on five benchmark datasets, Micro-Act consistently achieves\\nsignificant increase in QA accuracy over state-of-the-art baselines across all\\n5 datasets and 3 conflict types, especially in temporal and semantic types\\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\\nrobust performance on non-conflict questions simultaneously, highlighting its\\npractical value in real-world RAG applications.   \n",
              "5                                                         Forecasting is a critical task in decision-making across numerous domains.\\nWhile historical numerical data provide a start, they fail to convey the\\ncomplete context for reliable and accurate predictions. Human forecasters\\nfrequently rely on additional information, such as background knowledge and\\nconstraints, which can efficiently be communicated through natural language.\\nHowever, in spite of recent progress with LLM-based forecasters, their ability\\nto effectively integrate this textual information remains an open question. To\\naddress this, we introduce \"Context is Key\" (CiK), a time-series forecasting\\nbenchmark that pairs numerical data with diverse types of carefully crafted\\ntextual context, requiring models to integrate both modalities; crucially,\\nevery task in CiK requires understanding textual context to be solved\\nsuccessfully. We evaluate a range of approaches, including statistical models,\\ntime series foundation models, and LLM-based forecasters, and propose a simple\\nyet effective LLM prompting method that outperforms all other tested methods on\\nour benchmark. Our experiments highlight the importance of incorporating\\ncontextual information, demonstrate surprising performance when using LLM-based\\nforecasting models, and also reveal some of their critical shortcomings. This\\nbenchmark aims to advance multimodal forecasting by promoting models that are\\nboth accurate and accessible to decision-makers with varied technical\\nexpertise. The benchmark can be visualized at\\nhttps://servicenow.github.io/context-is-key-forecasting/v0/.   \n",
              "\n",
              "                                 url               categories  \n",
              "0  http://arxiv.org/pdf/2506.05334v1    [cs.CL, cs.IR, cs.LG]  \n",
              "1  http://arxiv.org/pdf/2503.24102v2                  [cs.CL]  \n",
              "2  http://arxiv.org/pdf/2506.05295v1  [cs.LG, cs.AI, stat.ML]  \n",
              "3  http://arxiv.org/pdf/2412.04119v3                  [cs.CL]  \n",
              "4  http://arxiv.org/pdf/2506.05278v1           [cs.CL, cs.AI]  \n",
              "5  http://arxiv.org/pdf/2410.18959v4  [cs.LG, cs.AI, stat.ML]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1fbcb756-3ecb-4f37-b915-9e593de26769\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>url</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-05 17:59:26+00:00</td>\n",
              "      <td>Search Arena: Analyzing Search-Augmented LLMs</td>\n",
              "      <td>Search-augmented language models combine web search with Large Language\\nModels (LLMs) to improve response groundedness and freshness. However,\\nanalyzing these systems remains challenging: existing datasets are limited in\\nscale and narrow in scope, often constrained to static, single-turn,\\nfact-checking questions. In this work, we introduce Search Arena, a\\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\\ndiverse intents and languages, and contains full system traces with around\\n12,000 human preference votes. Our analysis reveals that user preferences are\\ninfluenced by the number of citations, even when the cited content does not\\ndirectly support the attributed claims, uncovering a gap between perceived and\\nactual credibility. Furthermore, user preferences vary across cited sources,\\nrevealing that community-driven platforms are generally preferred and static\\nencyclopedic sources are not always appropriate and reliable. To assess\\nperformance across different settings, we conduct cross-arena analyses by\\ntesting search-augmented LLMs in a general-purpose chat environment and\\nconventional LLMs in search-intensive settings. We find that web search does\\nnot degrade and may even improve performance in non-search settings; however,\\nthe quality in search settings is significantly affected if solely relying on\\nthe model's parametric knowledge. We open-sourced the dataset to support future\\nresearch in this direction. Our dataset and code are available at:\\nhttps://github.com/lmarena/search-arena.</td>\n",
              "      <td>http://arxiv.org/pdf/2506.05334v1</td>\n",
              "      <td>[cs.CL, cs.IR, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-03-31 13:56:03+00:00</td>\n",
              "      <td>Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?</td>\n",
              "      <td>Low-Resource Languages (LRLs) present significant challenges in natural\\nlanguage processing due to their limited linguistic resources and\\nunderrepresentation in standard datasets. While recent advances in Large\\nLanguage Models (LLMs) and Neural Machine Translation have substantially\\nimproved translation capabilities for high-resource languages, performance\\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\\nresource-constrained scenarios. This paper systematically evaluates current\\nLLMs in 200 languages using the FLORES-200 benchmark and demonstrates their\\nlimitations in LRL translation capability. We also explore alternative data\\nsources, including news articles and bilingual dictionaries, and demonstrate\\nhow knowledge distillation from large pre-trained teacher models can\\nsignificantly improve the performance of small LLMs on LRL translation tasks.\\nFor example, this approach increases EN-&gt;LB with the LLM-as-a-Judge score on\\nthe validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine\\ndifferent fine-tuning configurations, providing practical insights on optimal\\ndata scale, training efficiency, and the preservation of generalization\\ncapabilities of models under study.</td>\n",
              "      <td>http://arxiv.org/pdf/2503.24102v2</td>\n",
              "      <td>[cs.CL]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-06-05 17:48:19+00:00</td>\n",
              "      <td>Sample Complexity and Representation Ability of Test-time Scaling Paradigms</td>\n",
              "      <td>Test-time scaling paradigms have significantly advanced the capabilities of\\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\\ntheoretical understanding of the sample efficiency of various test-time\\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\\nremains limited. In this work, we first establish a separation result between\\ntwo repeated sampling strategies: self-consistency requires\\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta &lt; 1$ denotes the probability gap\\nbetween the correct and second most likely answers. Next, we present an\\nexpressiveness result for the self-correction approach with verifier feedback:\\nit enables Transformers to simulate online learning over a pool of experts at\\ntest time. Therefore, a single Transformer architecture can provably solve\\nmultiple tasks without prior knowledge of the specific task associated with a\\nuser query, extending the representation theory of Transformers from\\nsingle-task to multi-task settings. Finally, we empirically validate our\\ntheoretical results, demonstrating the practical effectiveness of\\nself-correction methods.</td>\n",
              "      <td>http://arxiv.org/pdf/2506.05295v1</td>\n",
              "      <td>[cs.LG, cs.AI, stat.ML]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-12-05 12:37:27+00:00</td>\n",
              "      <td>GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice Question Answering</td>\n",
              "      <td>Pre-trained Language Models (PLMs) have shown remarkable performances in\\nrecent years, setting a new paradigm for NLP research and industry. The legal\\ndomain has received some attention from the NLP community partly due to its\\ntextual nature. Some tasks from this domain are represented by\\nquestion-answering (QA) tasks. This work explores the legal domain\\nMultiple-Choice QA (MCQA) for a low-resource language. The contribution of this\\nwork is multi-fold. We first introduce JuRO, the first openly available\\nRomanian legal MCQA dataset, comprising three different examinations and a\\nnumber of 10,836 total questions. Along with this dataset, we introduce CROL,\\nan organized corpus of laws that has a total of 93 distinct documents with\\ntheir modifications from 763 time spans, that we leveraged in this work for\\nInformation Retrieval (IR) techniques. Moreover, we are the first to propose\\nLaw-RoG, a Knowledge Graph (KG) for the Romanian language, and this KG is\\nderived from the aforementioned corpus. Lastly, we propose a novel approach for\\nMCQA, Graph Retrieval Augmented by Facts (GRAF), which achieves competitive\\nresults with generally accepted SOTA methods and even exceeds them in most\\nsettings.</td>\n",
              "      <td>http://arxiv.org/pdf/2412.04119v3</td>\n",
              "      <td>[cs.CL]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-06-05 17:33:02+00:00</td>\n",
              "      <td>Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning</td>\n",
              "      <td>Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\\nConflicts, where retrieved external knowledge contradicts the inherent,\\nparametric knowledge of large language models (LLMs). It adversely affects\\nperformance on downstream tasks such as question answering (QA). Existing\\napproaches often attempt to mitigate conflicts by directly comparing two\\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\\nextraneous or lengthy contexts, ultimately hindering their ability to identify\\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\\nframework with a hierarchical action space that automatically perceives context\\ncomplexity and adaptively decomposes each knowledge source into a sequence of\\nfine-grained comparisons. These comparisons are represented as actionable\\nsteps, enabling reasoning beyond the superficial context. Through extensive\\nexperiments on five benchmark datasets, Micro-Act consistently achieves\\nsignificant increase in QA accuracy over state-of-the-art baselines across all\\n5 datasets and 3 conflict types, especially in temporal and semantic types\\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\\nrobust performance on non-conflict questions simultaneously, highlighting its\\npractical value in real-world RAG applications.</td>\n",
              "      <td>http://arxiv.org/pdf/2506.05278v1</td>\n",
              "      <td>[cs.CL, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2024-10-24 17:56:08+00:00</td>\n",
              "      <td>Context is Key: A Benchmark for Forecasting with Essential Textual Information</td>\n",
              "      <td>Forecasting is a critical task in decision-making across numerous domains.\\nWhile historical numerical data provide a start, they fail to convey the\\ncomplete context for reliable and accurate predictions. Human forecasters\\nfrequently rely on additional information, such as background knowledge and\\nconstraints, which can efficiently be communicated through natural language.\\nHowever, in spite of recent progress with LLM-based forecasters, their ability\\nto effectively integrate this textual information remains an open question. To\\naddress this, we introduce \"Context is Key\" (CiK), a time-series forecasting\\nbenchmark that pairs numerical data with diverse types of carefully crafted\\ntextual context, requiring models to integrate both modalities; crucially,\\nevery task in CiK requires understanding textual context to be solved\\nsuccessfully. We evaluate a range of approaches, including statistical models,\\ntime series foundation models, and LLM-based forecasters, and propose a simple\\nyet effective LLM prompting method that outperforms all other tested methods on\\nour benchmark. Our experiments highlight the importance of incorporating\\ncontextual information, demonstrate surprising performance when using LLM-based\\nforecasting models, and also reveal some of their critical shortcomings. This\\nbenchmark aims to advance multimodal forecasting by promoting models that are\\nboth accurate and accessible to decision-makers with varied technical\\nexpertise. The benchmark can be visualized at\\nhttps://servicenow.github.io/context-is-key-forecasting/v0/.</td>\n",
              "      <td>http://arxiv.org/pdf/2410.18959v4</td>\n",
              "      <td>[cs.LG, cs.AI, stat.ML]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fbcb756-3ecb-4f37-b915-9e593de26769')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1fbcb756-3ecb-4f37-b915-9e593de26769 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1fbcb756-3ecb-4f37-b915-9e593de26769');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c562f900-3ab7-4fab-8e6a-b9271569635e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c562f900-3ab7-4fab-8e6a-b9271569635e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c562f900-3ab7-4fab-8e6a-b9271569635e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-10-24 17:56:08+00:00\",\n        \"max\": \"2025-06-05 17:59:26+00:00\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"2025-06-05 17:59:26+00:00\",\n          \"2025-03-31 13:56:03+00:00\",\n          \"2024-10-24 17:56:08+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Search Arena: Analyzing Search-Augmented LLMs\",\n          \"Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?\",\n          \"Context is Key: A Benchmark for Forecasting with Essential Textual Information\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Search-augmented language models combine web search with Large Language\\nModels (LLMs) to improve response groundedness and freshness. However,\\nanalyzing these systems remains challenging: existing datasets are limited in\\nscale and narrow in scope, often constrained to static, single-turn,\\nfact-checking questions. In this work, we introduce Search Arena, a\\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\\ndiverse intents and languages, and contains full system traces with around\\n12,000 human preference votes. Our analysis reveals that user preferences are\\ninfluenced by the number of citations, even when the cited content does not\\ndirectly support the attributed claims, uncovering a gap between perceived and\\nactual credibility. Furthermore, user preferences vary across cited sources,\\nrevealing that community-driven platforms are generally preferred and static\\nencyclopedic sources are not always appropriate and reliable. To assess\\nperformance across different settings, we conduct cross-arena analyses by\\ntesting search-augmented LLMs in a general-purpose chat environment and\\nconventional LLMs in search-intensive settings. We find that web search does\\nnot degrade and may even improve performance in non-search settings; however,\\nthe quality in search settings is significantly affected if solely relying on\\nthe model's parametric knowledge. We open-sourced the dataset to support future\\nresearch in this direction. Our dataset and code are available at:\\nhttps://github.com/lmarena/search-arena.\",\n          \"Low-Resource Languages (LRLs) present significant challenges in natural\\nlanguage processing due to their limited linguistic resources and\\nunderrepresentation in standard datasets. While recent advances in Large\\nLanguage Models (LLMs) and Neural Machine Translation have substantially\\nimproved translation capabilities for high-resource languages, performance\\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\\nresource-constrained scenarios. This paper systematically evaluates current\\nLLMs in 200 languages using the FLORES-200 benchmark and demonstrates their\\nlimitations in LRL translation capability. We also explore alternative data\\nsources, including news articles and bilingual dictionaries, and demonstrate\\nhow knowledge distillation from large pre-trained teacher models can\\nsignificantly improve the performance of small LLMs on LRL translation tasks.\\nFor example, this approach increases EN->LB with the LLM-as-a-Judge score on\\nthe validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine\\ndifferent fine-tuning configurations, providing practical insights on optimal\\ndata scale, training efficiency, and the preservation of generalization\\ncapabilities of models under study.\",\n          \"Forecasting is a critical task in decision-making across numerous domains.\\nWhile historical numerical data provide a start, they fail to convey the\\ncomplete context for reliable and accurate predictions. Human forecasters\\nfrequently rely on additional information, such as background knowledge and\\nconstraints, which can efficiently be communicated through natural language.\\nHowever, in spite of recent progress with LLM-based forecasters, their ability\\nto effectively integrate this textual information remains an open question. To\\naddress this, we introduce \\\"Context is Key\\\" (CiK), a time-series forecasting\\nbenchmark that pairs numerical data with diverse types of carefully crafted\\ntextual context, requiring models to integrate both modalities; crucially,\\nevery task in CiK requires understanding textual context to be solved\\nsuccessfully. We evaluate a range of approaches, including statistical models,\\ntime series foundation models, and LLM-based forecasters, and propose a simple\\nyet effective LLM prompting method that outperforms all other tested methods on\\nour benchmark. Our experiments highlight the importance of incorporating\\ncontextual information, demonstrate surprising performance when using LLM-based\\nforecasting models, and also reveal some of their critical shortcomings. This\\nbenchmark aims to advance multimodal forecasting by promoting models that are\\nboth accurate and accessible to decision-makers with varied technical\\nexpertise. The benchmark can be visualized at\\nhttps://servicenow.github.io/context-is-key-forecasting/v0/.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"http://arxiv.org/pdf/2506.05334v1\",\n          \"http://arxiv.org/pdf/2503.24102v2\",\n          \"http://arxiv.org/pdf/2410.18959v4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example abstract from API\n",
        "\n",
        "summary = df['summary'][0]\n",
        "\n",
        "summarizer = pipeline('summarization', model = 'facebook/bart-large-cnn')\n",
        "summarization_result = summarizer(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAlKVHmGjdI5",
        "outputId": "a923e18c-4e96-4e82-f731-fa3f8015e4a4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarization_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh-5_yidk_wb",
        "outputId": "c0ba6ae3-c80f-45ca-8cc3-b2c8f893412a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'Search Arena is a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations.'}]\n"
          ]
        }
      ]
    }
  ]
}